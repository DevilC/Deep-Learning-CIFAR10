{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Load Libraries</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Load CIFAR-10 dataset</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000L, 3L, 32L, 32L)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    " \n",
    "def load_dataset():\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    print 'X_train shape:', X_train.shape\n",
    "    print X_train.shape[0], 'train samples'\n",
    "    print X_test.shape[0], 'test samples'\n",
    " \n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    " \n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    " \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Create a model with 4 convolutional layers and 2 fully-connected layers</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(channels, image_rows, image_cols, lr, decay, momentum):\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape = (channels, image_rows, image_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_network(3, 32, 32, 0.01, 1e-6, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Defining call backs</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "checkpoint = ModelCheckpoint(filepath = 'best_model_0718.hdf5', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Fit the model with 10% validation split</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 2.0382 Epoch 00000: val_loss improved from inf to 1.81212, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2305s - loss: 2.0377 - val_loss: 1.8121\n",
      "Epoch 2/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.6530 Epoch 00001: val_loss improved from 1.81212 to 1.44449, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2374s - loss: 1.6529 - val_loss: 1.4445\n",
      "Epoch 3/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.4458Epoch 00002: val_loss improved from 1.44449 to 1.33940, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2232s - loss: 1.4460 - val_loss: 1.3394\n",
      "Epoch 4/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.3230Epoch 00003: val_loss improved from 1.33940 to 1.21940, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2232s - loss: 1.3232 - val_loss: 1.2194\n",
      "Epoch 5/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.2270Epoch 00004: val_loss improved from 1.21940 to 1.08943, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2296s - loss: 1.2272 - val_loss: 1.0894\n",
      "Epoch 6/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.1511 Epoch 00005: val_loss improved from 1.08943 to 1.00942, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2317s - loss: 1.1510 - val_loss: 1.0094\n",
      "Epoch 7/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.0771 Epoch 00006: val_loss improved from 1.00942 to 0.99380, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2327s - loss: 1.0771 - val_loss: 0.9938\n",
      "Epoch 8/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 1.0205 Epoch 00007: val_loss improved from 0.99380 to 0.91320, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2314s - loss: 1.0204 - val_loss: 0.9132\n",
      "Epoch 9/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.9723 Epoch 00008: val_loss improved from 0.91320 to 0.91129, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2327s - loss: 0.9725 - val_loss: 0.9113\n",
      "Epoch 10/100\n",
      "44928/45000 [============================>.] - ETA: 8s - loss: 0.9329 Epoch 00009: val_loss improved from 0.91129 to 0.83695, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 5226s - loss: 0.9328 - val_loss: 0.8370\n",
      "Epoch 11/100\n",
      "44928/45000 [============================>.] - ETA: 10s - loss: 0.8880Epoch 00010: val_loss improved from 0.83695 to 0.77608, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 6509s - loss: 0.8878 - val_loss: 0.7761\n",
      "Epoch 12/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.8541Epoch 00011: val_loss improved from 0.77608 to 0.75537, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2232s - loss: 0.8539 - val_loss: 0.7554\n",
      "Epoch 13/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.8214Epoch 00012: val_loss improved from 0.75537 to 0.72552, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2264s - loss: 0.8214 - val_loss: 0.7255\n",
      "Epoch 14/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.7889Epoch 00013: val_loss improved from 0.72552 to 0.70837, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2286s - loss: 0.7891 - val_loss: 0.7084\n",
      "Epoch 15/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.7683Epoch 00014: val_loss improved from 0.70837 to 0.68952, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2275s - loss: 0.7683 - val_loss: 0.6895\n",
      "Epoch 16/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.7478 Epoch 00015: val_loss improved from 0.68952 to 0.68158, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2314s - loss: 0.7477 - val_loss: 0.6816\n",
      "Epoch 17/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.7225 Epoch 00016: val_loss did not improve\n",
      "45000/45000 [==============================] - 2296s - loss: 0.7224 - val_loss: 0.7220\n",
      "Epoch 18/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6981Epoch 00017: val_loss improved from 0.68158 to 0.66628, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2279s - loss: 0.6981 - val_loss: 0.6663\n",
      "Epoch 19/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6809Epoch 00018: val_loss improved from 0.66628 to 0.63095, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2236s - loss: 0.6809 - val_loss: 0.6310\n",
      "Epoch 20/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6627Epoch 00019: val_loss improved from 0.63095 to 0.62797, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2231s - loss: 0.6625 - val_loss: 0.6280\n",
      "Epoch 21/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6496Epoch 00020: val_loss improved from 0.62797 to 0.61915, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2231s - loss: 0.6495 - val_loss: 0.6192\n",
      "Epoch 22/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6263Epoch 00021: val_loss did not improve\n",
      "45000/45000 [==============================] - 2232s - loss: 0.6261 - val_loss: 0.6532\n",
      "Epoch 23/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6185Epoch 00022: val_loss did not improve\n",
      "45000/45000 [==============================] - 2230s - loss: 0.6185 - val_loss: 0.6221\n",
      "Epoch 24/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.6021Epoch 00023: val_loss improved from 0.61915 to 0.59977, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2233s - loss: 0.6017 - val_loss: 0.5998\n",
      "Epoch 25/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.5850Epoch 00024: val_loss improved from 0.59977 to 0.58991, saving model to best_model_0718.hdf5\n",
      "45000/45000 [==============================] - 2232s - loss: 0.5848 - val_loss: 0.5899\n",
      "Epoch 26/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.5789Epoch 00025: val_loss did not improve\n",
      "45000/45000 [==============================] - 2230s - loss: 0.5790 - val_loss: 0.5956\n",
      "Epoch 27/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.5567Epoch 00026: val_loss did not improve\n",
      "45000/45000 [==============================] - 2234s - loss: 0.5566 - val_loss: 0.6096\n",
      "Epoch 28/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.5424Epoch 00027: val_loss did not improve\n",
      "45000/45000 [==============================] - 2250s - loss: 0.5425 - val_loss: 0.5939\n",
      "Epoch 29/100\n",
      "44928/45000 [============================>.] - ETA: 3s - loss: 0.5296Epoch 00028: val_loss did not improve\n",
      "Epoch 00028: early stopping\n",
      "45000/45000 [==============================] - 2249s - loss: 0.5296 - val_loss: 0.6059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xc7df438>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch=100, batch_size=128, validation_split=0.1, verbose=1, callbacks=[checkpoint, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Save the model and the weights</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_model = model.to_json()\n",
    "with open(\"model_final.json\", \"w\") as json_file:\n",
    "    json_file.write(json_model)\n",
    "    \n",
    "model.save_weights('model_weights_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Load the model and the weights saved using checkpoint</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = open('model_final.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model1 = model_from_json(loaded_model_json)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model1.load_weights(\"best_model_0718.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Verify the accuracy of the model on test set</H4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 95s    \n",
      "Test Loss: 0.638751012707\n",
      "10000/10000 [==============================] - 102s   \n",
      "Test Accuracy: 0.7786\n"
     ]
    }
   ],
   "source": [
    "test_loss = model1.evaluate(X_test, y_test, batch_size = 64, verbose=1)\n",
    "print 'Test Loss:', test_loss\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "Y_test = np.dot(y_test, classes).astype(int)\n",
    "\n",
    "y_predict1 = model1.predict_classes(X_test, verbose = 1)\n",
    "test_accuracy1 = accuracy_score(Y_test, y_predict1)\n",
    "print 'Test Accuracy:', test_accuracy1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 97s    \n",
      "Test Loss: 0.636110443497\n",
      "10000/10000 [==============================] - 103s   \n",
      "Test Accuracy: 0.7855\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(X_test, y_test, batch_size = 64, verbose=1)\n",
    "print 'Test Loss:', test_loss\n",
    "\n",
    "y_predict = model.predict_classes(X_test, verbose = 1)\n",
    "test_accuracy = accuracy_score(Y_test, y_predict)\n",
    "print 'Test Accuracy:', test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
